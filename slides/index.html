<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <title>reveal.js</title>

    <link rel="stylesheet" href="dist/reset.css" />
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/black.css" />

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css" />
  </head>

  <style>
    .fragment.blur {
      filter: blur(5px);
    }

    .fragment.blur.visible {
      filter: none;
    }
  </style>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h1>NLP Hate Speech</h1>
        </section>

        <section>
          <h2>Text Pre-processing</h2>

          <section>
            <h3>Initially pre-processed text data</h3>

            <div style="display: flex; gap: 10px">
              <p class="fragment fade-in" data-autoslide="1000">
                @Random_username
              </p>

              <p class="fragment fade-left" data-autoslide="600">= @USER</p>
            </div>

            <div style="display: flex; gap: 10px">
              <p class="fragment fade-in" data-autoslide="1000">
                u/Random_username
              </p>

              <p class="fragment fade-left" style="color: red">
                = u/Random_username
              </p>
            </div>

            <div style="display: flex; gap: 10px">
              <p class="fragment fade-in" data-autoslide="1000">
                https://www.reddit.com/r/Random_username
              </p>

              <p class="fragment fade-left">= URL</p>
            </div>
          </section>

          <section>
            <div style="display: flex; gap: 40px; align-items: center">
              <h3>Tokenization</h3>

              <iframe
                data-src="https://spacy.io/usage/linguistic-features#pos-tagging"
                data-preload
                style="width: 100%; min-height: 500px"
              ></iframe>
            </div>
          </section>

          <section>
            <h3>Sanitize Methods</h3>

            <p class="fragment custom blur">Lowercase</p>
            <p class="fragment custom blur">Stop Words</p>
            <p class="fragment custom blur">Remove Punctuation</p>
            <p class="fragment custom blur">Lemmatize</p>
            <p class="fragment custom blur">Remove Duplicates</p>
          </section>
          <section data-auto-animate>
            <pre data-id="code-animation">
               <code data-trim style="font-size: 12px; line-height: 1.2;" data-line-numbers data-ln-start-from="">
              Datasets(TRAIN).remove_dots().lemmatize().remove_stop_words().remove_duplicates().lowercase()
               </code>
              </pre>
          </section>

          <section data-auto-animate>
            <pre data-id="code-animation">
               <code data-trim style="font-size: 12px; line-height: 1.2;" data-line-numbers="2" data-ln-start-from="">
              Datasets(TRAIN).remove_dots().lemmatize().remove_stop_words().remove_duplicates().lowercase()
              Datasets(TRAIN).lowercase().remove_dots().lemmatize().remove_stop_words().remove_duplicates()
               </code>
              </pre>
          </section>
          <section data-visibility="uncounted">
            <pre>

           <code data-trim style="font-size: 12px; line-height: 1.2;" >
              def remove_dots(self):
                  """remove all punctuation"""

                  # try getting the dataset variation from cache
                  self.dataset_type = self.dataset_type + "_remove-dots"
                  if self._try_load_from_disk():
                      return self

                  method: Callable[[list[Token]], list[Token]] = lambda lst: [
                      x for x in lst if not x.pos_ == "PUNCT"]
                  self.dataset = self._sanitize_dataset(self.dataset, method)

                  # save to disk for quicker execution next time
                  self._save_to_disk()
                  return self

              def remove_stop_words(self):
                  """remove the most common words in the danish language"""
                  self.dataset_type = self.dataset_type + "_remove-stop-words"
                  if self._try_load_from_disk():
                      return self

                  method: Callable[[list[Token]], list[Token]] = lambda lst: [
                      x for x in lst if not x.is_stop]
                  self.dataset = self._sanitize_dataset(self.dataset, method)

                  self._save_to_disk()
                  return self

              def lemmatize(self):
                  """group words together and convert to simplest form (see: https://en.wikipedia.org/wiki/Lemmatization)"""
                  self.dataset_type = self.dataset_type + "_lemmatize"
                  if self._try_load_from_disk():
                      return self

                  method: Callable[[list[Token]], list[Token]] = lambda lst: [x for x in self.nlp(" ".join([
                      x.lemma_ for x in lst]))]
                  self.dataset = self._sanitize_dataset(self.dataset, method)
                  self._save_to_disk()
                  return self

              def lowercase(self):
                  """lowercase wuhu"""
                  self.dataset_type = self.dataset_type + "_lowercase"
                  if self._try_load_from_disk():
                      return self

                  method: Callable[[list[Token]], list[Token]] = lambda lst: [x for x in self.nlp(" ".join([
                      x.lower_ for x in lst]))]
                  self.dataset = self._sanitize_dataset(self.dataset, method)
                  self._save_to_disk()
                  return self

              def remove_duplicates(self):
                  """extracts unique words from the dataset"""
                  self.dataset_type = self.dataset_type + "_remove_duplicates"
                  if self._try_load_from_disk():
                      return self

                  method: Callable[[list[Token]], list[Token]
                                  ] = lambda lst: list(dict([(i.text, i) for i in lst]).values())
                  self.dataset = self._sanitize_dataset(self.dataset, method)
                  self._save_to_disk()
                  return self
           </code>
            </pre>
          </section>
        </section>

        <section>
          <h2>Models</h2>
          <ul>
            <li>Naive Bayes</li>
            <li>Logistic Regression</li>
            <li>Support Vector Machine</li>
          </ul>
        </section>

        <section>
          <h2>Naive Bayes</h2>
          <section>
            <pre>
           <code data-autoslide="400" data-trim data-line-numbers="|7,9-13,31" style="font-size: 12px; line-height: 1.2;" >
              class NaiveBayes(MLAlgorithm):  # pragma: no cover
                  def __init__(
                      self,
                      dataset: Dataset,
                      model_name="naive-bayes",
                      variation_name=None,
                      k_factor: float = 1,
                  ) -> None:  # pragma: no cover
                      if k_factor != 1:
                          if variation_name is None:
                              variation_name = f"add-k-{k_factor}"
                          else:
                              variation_name = f"add-k-{k_factor}_" + variation_name

                      super().__init__(dataset, model_name, variation_name)  # type: ignore
                      # base chance based on the split in classes in the dataset
                      self.logprior = {}
                      # Chance for each word to belong to each class
                      self.loglikelihood = {}

                      # amount of instances aka. comments/sentences in the dataset
                      self.n_instances = len(self.dataset[OFF]) + len(self.dataset[NOT])

                      # creates a set of words in the dataset
                      self.vocabulary: set[Token] = set()
                      for comment in dataset.to_list():
                          self.vocabulary.update(comment)

                      self.train_data = TrainData(self.name)

                      self.k_factor = k_factor
           </code>
            </pre>
            <p class="fragment fade-in">Laplace Smoothing</p>
          </section>

          <section>
            <pre>
           <code data-trim data-line-numbers="1,7-8" style="font-size: 14px; line-height: 1.2;" >
            def remove_duplicates(self):
                """extracts unique words from the dataset"""
                self.dataset_type = self.dataset_type + "_remove_duplicates"
                if self._try_load_from_disk():
                    return self

                method: Callable[[list[Token]], list[Token]
                                ] = lambda lst: list(dict([(i.text, i) for i in lst]).values())
                self.dataset = self._sanitize_dataset(self.dataset, method)
                self._save_to_disk()
                return self
           </code>
            </pre>
            <p>Binary Naive Bayes</p>
          </section>
        </section>

        <section>
          <h2>Logistic Regression</h2>

          <section>
            <pre>
           <code data-trim data-line-numbers="1,4-6,14" style="font-size: 14px; line-height: 1.2;" >
            class LogisticRegression(MLAlgorithm):
            def __init__(self, dataset: DatasetDict, variation_name=None) -> None:
                super().__init__(dataset, "logistic-regression", variation_name)
                self.hateful_words: set = set(
                    pd.read_csv("./hurtlex_DA.tsv", sep="\t")["lemma"]
                )

                self.data_length = len(self.dataset[OFF]) + len(self.dataset[NOT])
                self.variation_name = ""
                self.bias_term = 0
                self.weights = [0, 0, 0, 0, 0, 0]

                self.positive_words: set = set()
                pos_words = open("data/sentiment-lexicons/positive_words_da.txt", "r")
                while True:
                    word = pos_words.readline()
                    if not word:
                        break
                    self.positive_words.add(word[:-1])
                pos_words.close()
           </code>
          </pre>
          </section>
          <section>
            <p>Negative words</p>
            <pre>
           <code data-trim data-line-numbers="" style="font-size: 16px; line-height: 1.2;" >
                id	    pos	category	stereotype	lemma	              level
                DA2053	n	  re	      no	        mord	              inclusive
                DA1772	n	  re	      no	        cosa nostra	        conservative
                DA1139	n	  cds	      no	        peon	              inclusive
                DA2176	n	  cds	      no	        wank	              conservative
                DA1486	n	  cds     	no	        den store uvaskede	conservative
                DA1259	n	  pa	      yes	        fiskehandler	      inclusive
                DA639	  n	  an	      no	        rappenskralde	      conservative
                DA200	  n	  svp	      no	        gnier	              inclusive
                DA1772	n	  cds	      no	        cosa nostra	        inclusive
                DA831	  n	  ddp	      yes	        gossiper	          conservative
                DA988	  n	  cds	      no	        demagogi	          conservative
                ...
           </code>
          </pre>
          <p>Hurtlex DA</p>
          </section>

          <section>
            <p>Positive words</p>
            <ul style="list-style: none">
              <li>som</li>
              <li>under</li>
              <li>mod</li>
              <li>store</li>
              <li>stor</li>
              <li>side</li>
              <li>helt</li>
            </ul>
            <ul style="list-style: none">
              <li>ny</li>
              <li>bedste</li>
              <li>arbejde</li>
              <li>godt</li>
              <li>ses</li>
              <li>førte</li>
              <li>...</li>
            </ul>
          </section>

          <section>
            <h3>N-Grams</h3>
            <p><span style="color: red;">Word</span> N-Grams</p>
            <p><span style="color: red;">Cha</span><span style="color: white;">rac</span><span style="color: red;">ter</span> N-Grams</p>

          </section>
        </section>

        <section>
         <h2>Support Vector Machine</h2>

            <pre>
           <code data-trim data-line-numbers="1|20-34|23-30" style="font-size: 12px; line-height: 1.2;" >
            from sklearn.svm import SVC

            class SVM(MLAlgorithm):
                def __init__(self, dataset: DatasetDict, variation_name=None) -> None:
                    super().__init__(dataset, "svm", variation_name)

                    self.data = {}
                    off = [(x, "OFF") for x in self.dataset["OFF"]]
                    not_off = [(x, "NOT") for x in self.dataset["NOT"]]
                    self.list = off + not_off

                    shuffle(self.list)

                    self.X = [x[0].text for x in self.list]
                    self.y = [x[1] for x in self.list]

                    self.df = pd.DataFrame(self.data)

                    self.variation_name = ""
                    self.svm_model = make_pipeline(
                        FeatureUnion(
                            [
                                (
                                    "word_tfidf",
                                    TfidfVectorizer(analyzer="word", ngram_range=(1, 2)),
                                ),
                                (
                                    "char_tfidf",
                                    TfidfVectorizer(analyzer="char", ngram_range=(2, 4)),
                                ),
                            ]
                        ),
                        SVC(kernel="linear", C=10),
                    )
                    self.is_trained = False
           </code>
            </pre>
        </section>

        <section>
          <h2>
            Benchmarking
          </h2>

          <ul style="list-style: none;">
            <li>
              Accuracy
            </li>
            <li>Precision</li>
            <li>Recall</li>
            <li>F1</li>
          </ul>
        </section>

        <section>
          <h2>Results</h2>

          <section>

          </section>
        </section>
      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
      });
    </script>
  </body>
</html>
